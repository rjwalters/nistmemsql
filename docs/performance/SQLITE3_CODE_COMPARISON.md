# SQLite3 vs vibesql: Detailed Code Comparison by Microbenchmark

**Date**: 2025-11-06
**Purpose**: Compare implementation approaches between vibesql and SQLite3 for each microbenchmark operation to identify optimization opportunities

**Benchmark Categories**:
1. INSERT operations (1K, 10K, 100K rows)
2. UPDATE operations (1K, 10K, 100K rows)
3. DELETE operations (1K, 10K, 100K rows)
4. SELECT with WHERE clause (10% filtering)
5. Aggregate operations (COUNT, SUM, AVG)

---

## Table of Contents

1. [INSERT Operations](#1-insert-operations)
2. [UPDATE Operations](#2-update-operations)
3. [DELETE Operations](#3-delete-operations)
4. [SELECT with WHERE](#4-select-with-where-clause)
5. [Aggregate Operations](#5-aggregate-operations-count-sum-avg)
6. [Summary & Optimization Opportunities](#summary--optimization-opportunities)

---

## 1. INSERT Operations

### Benchmark Profile
- **Test**: Insert N rows in a tight loop
- **Scales**: 1K, 10K, 100K rows
- **Expected bottlenecks**: Validation overhead, constraint checking, memory allocations

### vibesql Implementation

**File**: `crates/executor/src/insert/execution.rs`

**Architecture**:
```
execute_insert()
â”œâ”€â”€ Privilege check
â”œâ”€â”€ Resolve target columns
â”œâ”€â”€ Process insert source (VALUES or SELECT)
â”‚   â””â”€â”€ Bulk transfer optimization for INSERT...SELECT
â”œâ”€â”€ Validate row column counts
â””â”€â”€ Two-phase execution:
    â”œâ”€â”€ Phase 1: Validate ALL rows
    â”‚   â”œâ”€â”€ Build complete row with NULLs
    â”‚   â”œâ”€â”€ Evaluate expressions (literals, DEFAULT)
    â”‚   â”œâ”€â”€ Type coercion
    â”‚   â”œâ”€â”€ Apply DEFAULT values
    â”‚   â””â”€â”€ Validate constraints:
    â”‚       â”œâ”€â”€ NOT NULL
    â”‚       â”œâ”€â”€ PRIMARY KEY (batch duplicate check)
    â”‚       â”œâ”€â”€ UNIQUE (batch duplicate check)
    â”‚       â”œâ”€â”€ CHECK constraints
    â”‚       â””â”€â”€ FOREIGN KEY references
    â””â”€â”€ Phase 2: Insert ALL validated rows
        â””â”€â”€ storage::Database::insert_row() per row
```

**Key characteristics**:
- âœ… **Atomic batch validation**: All rows validated before any insertion
- âœ… **Bulk transfer optimization**: 10-50x faster for `INSERT...SELECT` with compatible schemas
- âš ï¸ **Per-row insertion**: Individual `insert_row()` calls even after batch validation
- âš ï¸ **Memory overhead**: Stores all validated rows in memory before insertion
- âš ï¸ **Multiple allocations**: Vec allocations for tracking PK values and UNIQUE values per batch

**Code snippet** (execution.rs:69-135):
```rust
// Two-phase: validation then insertion
let mut validated_rows = Vec::new();
let mut primary_key_values: Vec<Vec<types::SqlValue>> = Vec::new();

// Phase 1: Validate all rows
for value_exprs in &rows_to_insert {
    let mut full_row_values = vec![types::SqlValue::Null; schema.columns.len()];
    // ... expression evaluation, type coercion, defaults ...
    let validator = super::row_validator::RowValidator::new(...);
    let validation_result = validator.validate(&full_row_values)?;
    validated_rows.push(full_row_values);
}

// Phase 2: Insert all rows
for full_row_values in validated_rows {
    db.insert_row(&stmt.table_name, row)?;
    rows_inserted += 1;
}
```

### SQLite3 Implementation

**Files**:
- `src/insert.c` - Main INSERT logic
- `src/vdbeaux.c` - VDBE (Virtual Database Engine) bytecode execution
- `src/btree.c` - B-tree storage operations

**Architecture**:
```
sqlite3Insert()
â”œâ”€â”€ Parse and validate INSERT statement
â”œâ”€â”€ Generate VDBE bytecode program
â””â”€â”€ Execute bytecode:
    â”œâ”€â”€ OpenWrite - Open table B-tree cursor
    â”œâ”€â”€ For each row:
    â”‚   â”œâ”€â”€ MakeRecord - Pack row into byte array
    â”‚   â”œâ”€â”€ Constraint checks (integrated into B-tree insert)
    â”‚   â””â”€â”€ BtreeInsert - Direct B-tree node insertion
    â””â”€â”€ Close - Finalize transaction
```

**Key characteristics**:
- âœ… **Compiled bytecode**: INSERT converted to optimized bytecode program
- âœ… **Streaming insertion**: Rows inserted one-by-one without batching overhead
- âœ… **Zero-copy where possible**: MakeRecord packs directly into B-tree pages
- âœ… **B-tree integration**: Constraint checks integrated with B-tree operations
- âœ… **Write-ahead log (WAL)**: Batched I/O for disk writes (though in-memory benchmarks bypass this)

**Pseudocode** (simplified from insert.c):
```c
// Generate bytecode for INSERT
void sqlite3Insert(...) {
    // ... parse and setup ...

    // Generate VDBE instructions
    OpenWrite(cursor, tableRootPage);
    for (each value list) {
        MakeRecord(values, record);  // Pack into byte array
        Insert(cursor, record);       // B-tree insertion
    }
    Close(cursor);
}

// VDBE execution (vdbe.c)
case OP_Insert:
    BtreeInsert(cursor, record);  // Direct write to B-tree
    break;
```

### Performance Comparison

| Aspect | vibesql | SQLite3 | Winner | Notes |
|--------|---------|---------|--------|-------|
| **Execution model** | Two-phase (validate-then-insert) | Streaming bytecode | SQLite | vibesql validates entire batch before inserting |
| **Constraint checking** | Upfront batch validation | Integrated with B-tree insert | Mixed | vibesql catches all errors early; SQLite amortizes checks |
| **Memory allocations** | O(batch_size) for validation tracking | O(1) streaming | SQLite | vibesql allocates Vecs for validated rows, PK tracking, UNIQUE tracking |
| **Storage API calls** | N Ã— `insert_row()` | 1 Ã— `BtreeInsert()` per row | Tied | Both call storage layer N times |
| **Data structure** | HashMap-based storage | B-tree pages | SQLite | B-tree is cache-optimized, sequential |
| **Type coercion** | Per-column per-row | Compiled into bytecode | SQLite | vibesql runtime coercion vs compiled |

**vibesql overhead sources**:
1. **Batch validation allocations**: `Vec<Vec<SqlValue>>` for PK and UNIQUE tracking
2. **Two-phase execution**: Can't pipeline validation with insertion
3. **HashMap-based storage**: Less cache-friendly than B-tree pages
4. **Per-row API calls**: `insert_row()` has function call overhead

**Optimization opportunities**:
- âœ… **Already optimized**: Bulk transfer for INSERT...SELECT (10-50x improvement)
- ğŸ”§ **Pipeline validation**: Start inserting validated rows before batch completes
- ğŸ”§ **Reduce allocations**: Use arena allocator or pre-sized buffers
- ğŸ”§ **Storage batch API**: Add `insert_rows_batch()` to amortize HashMap operations
- ğŸ”§ **Bytecode compilation**: Pre-compile repetitive validation logic (future)

---

## 2. UPDATE Operations

### Benchmark Profile
- **Test**: Update all N rows with `SET value = value + 1 WHERE id = ?`
- **Scales**: 1K, 10K, 100K rows
- **Expected bottlenecks**: Row lookup, constraint validation, selective column updates

### vibesql Implementation

**File**: `crates/executor/src/update/mod.rs`

**Architecture**:
```
UpdateExecutor::execute()
â”œâ”€â”€ Privilege check (UPDATE privilege)
â”œâ”€â”€ Get table schema
â”œâ”€â”€ Create ExpressionEvaluator
â”œâ”€â”€ RowSelector: Select rows matching WHERE clause
â”‚   â”œâ”€â”€ Try primary key index optimization
â”‚   â””â”€â”€ Fallback to full table scan
â”œâ”€â”€ Two-phase execution:
â”‚   â”œâ”€â”€ Phase 1: Build updates list
â”‚   â”‚   â”œâ”€â”€ For each candidate row:
â”‚   â”‚   â”‚   â”œâ”€â”€ Check for PK updates â†’ verify no child references
â”‚   â”‚   â”‚   â”œâ”€â”€ Apply assignments to build new row
â”‚   â”‚   â”‚   â”œâ”€â”€ Validate constraints (NOT NULL, PK, UNIQUE, CHECK)
â”‚   â”‚   â”‚   â”œâ”€â”€ Validate FOREIGN KEYs
â”‚   â”‚   â”‚   â””â”€â”€ Collect (row_index, new_row, changed_columns)
â”‚   â””â”€â”€ Phase 2: Apply all updates
â”‚       â””â”€â”€ table.update_row_selective(index, new_row, changed_columns)
```

**Key characteristics**:
- âœ… **Selective column updates**: `update_row_selective()` only writes changed columns
- âœ… **Primary key index optimization**: Fast path for `WHERE id = ?` lookups
- âœ… **Two-phase semantics**: Correctly implements SQL's "read then write" requirement
- âš ï¸ **Per-row constraint validation**: `ConstraintValidator` created per row
- âš ï¸ **Per-row FK validation**: Foreign key checks per row even if FK columns unchanged
- âš ï¸ **Update list allocation**: Stores `(index, new_row, changed_columns)` tuples for entire batch

**Code snippet** (update/mod.rs:133-184):
```rust
// Phase 1: Build updates list
let mut updates: Vec<(usize, storage::Row, std::collections::HashSet<usize>)> = Vec::new();

for (row_index, row) in candidate_rows {
    // Check for child references if updating PK
    if updates_pk {
        ForeignKeyValidator::check_no_child_references(database, &stmt.table_name, &row)?;
    }

    // Apply assignments
    let (new_row, changed_columns) = value_updater.apply_assignments(&row, &stmt.assignments)?;

    // Validate constraints
    let constraint_validator = ConstraintValidator::new(schema);
    constraint_validator.validate_row(table, &stmt.table_name, row_index, &new_row, &row)?;

    // Validate foreign keys
    if !schema.foreign_keys.is_empty() {
        ForeignKeyValidator::validate_constraints(database, &stmt.table_name, &new_row.values)?;
    }

    updates.push((row_index, new_row, changed_columns));
}

// Phase 2: Apply updates
for (index, new_row, changed_columns) in updates {
    table_mut.update_row_selective(index, new_row, &changed_columns)?;
}
```

### SQLite3 Implementation

**Files**:
- `src/update.c` - Main UPDATE logic
- `src/vdbe.c` - Bytecode execution
- `src/btree.c` - B-tree updates

**Architecture**:
```
sqlite3Update()
â”œâ”€â”€ Parse and validate UPDATE statement
â”œâ”€â”€ Generate VDBE bytecode program
â””â”€â”€ Execute bytecode:
    â”œâ”€â”€ OpenWrite - Open table B-tree cursor
    â”œâ”€â”€ For each row matching WHERE:
    â”‚   â”œâ”€â”€ Seek/ScanNext - Find row via index or scan
    â”‚   â”œâ”€â”€ Column - Extract current column values
    â”‚   â”œâ”€â”€ Compute new values from SET expressions
    â”‚   â”œâ”€â”€ Constraint checks (if needed)
    â”‚   â””â”€â”€ Insert + Delete (or in-place update if possible)
    â””â”€â”€ Close
```

**Key characteristics**:
- âœ… **Index-aware execution**: Planner chooses optimal index for WHERE clause
- âœ… **In-place updates**: When row size doesn't change, update B-tree node in-place
- âœ… **Streaming execution**: Process one row at a time, no batch overhead
- âœ… **Compiled constraint checks**: Bytecode includes only necessary validation
- âœ… **Query planner optimization**: Cost-based decision for index usage

**Pseudocode** (simplified from update.c):
```c
// Generate bytecode for UPDATE
void sqlite3Update(...) {
    // ... parse and setup ...

    // Choose index for WHERE clause (if available)
    OpenRead(whereCursor, bestIndexRootPage);
    OpenWrite(tableCursor, tableRootPage);

    Rewind(whereCursor);
    while (NextRow(whereCursor)) {
        // Extract row
        rowid = Column(whereCursor, rowidColumn);
        Seek(tableCursor, rowid);

        // Compute new values
        for (each assignment) {
            newValue = Evaluate(expression);
        }

        // Update (in-place if possible)
        if (canUpdateInPlace) {
            Update(tableCursor, newRecord);
        } else {
            Delete(tableCursor, rowid);
            Insert(tableCursor, newRecord);
        }
    }
}
```

### Performance Comparison

| Aspect | vibesql | SQLite3 | Winner | Notes |
|--------|---------|---------|--------|-------|
| **Row lookup** | PK index or full scan | Query planner + index seek | SQLite | SQLite's planner chooses optimal access path |
| **Update strategy** | Always full row replacement | In-place if size unchanged | SQLite | In-place updates save memory copies |
| **Constraint validation** | Per-row ConstraintValidator creation | Compiled into bytecode | SQLite | vibesql creates new validator per row |
| **FK validation** | Always checked if FKs exist | Only if FK columns changed | SQLite | vibesql checks all FKs even if unchanged |
| **Two-phase execution** | Explicit batch + apply | Streaming one-at-a-time | vibesql | vibesql correctly implements SQL semantics; SQLite optimizes with assumptions |
| **Memory overhead** | O(updated_rows) for batch | O(1) streaming | SQLite | vibesql stores entire update list |

**vibesql overhead sources**:
1. **Per-row validator creation**: `ConstraintValidator::new()` and `ForeignKeyValidator` per row
2. **Unnecessary FK checks**: Validates all FKs even if unchanged columns
3. **Update list allocation**: Stores `(index, new_row, changed_columns)` tuples
4. **No in-place updates**: Always replaces entire row even for single column changes

**Optimization opportunities**:
- ğŸ”§ **Reuse validators**: Create `ConstraintValidator` once outside loop
- ğŸ”§ **Skip unchanged FK checks**: Only validate FKs if FK columns in `changed_columns`
- ğŸ”§ **In-place updates**: Implement storage-level in-place updates for same-size rows
- ğŸ”§ **Streaming execution**: Start applying updates before building complete list (if no PK updates)
- ğŸ”§ **Query planner**: Add cost-based index selection for WHERE clause

---

## 3. DELETE Operations

### Benchmark Profile
- **Test**: Delete N rows one-by-one with `DELETE FROM table WHERE id = ?`
- **Scales**: 1K, 10K, 100K rows
- **Expected bottlenecks**: Row lookup, referential integrity checks, tombstone management

### vibesql Implementation

**File**: `crates/executor/src/delete/executor.rs`

**Architecture**:
```
DeleteExecutor::execute()
â”œâ”€â”€ Privilege check (DELETE privilege)
â”œâ”€â”€ Fast path: DELETE without WHERE
â”‚   â””â”€â”€ Truncate optimization (if no child references)
â”œâ”€â”€ Get table schema
â”œâ”€â”€ Create ExpressionEvaluator
â”œâ”€â”€ Row selection:
â”‚   â”œâ”€â”€ Try primary key index lookup
â”‚   â”‚   â””â”€â”€ Single row extraction
â”‚   â””â”€â”€ Fallback to table scan
â”‚       â””â”€â”€ Collect (index, row) tuples
â”œâ”€â”€ Check referential integrity for each row
â”‚   â””â”€â”€ check_no_child_references()
â””â”€â”€ Delete rows using indices
    â””â”€â”€ table.delete_where(|row| indices.contains(index))
```

**Key characteristics**:
- âœ… **Primary key optimization**: Fast path for `WHERE id = ?` using PK index
- âœ… **TRUNCATE optimization**: Fast path for `DELETE FROM table` (100-1000x faster)
- âœ… **Two-phase execution**: Collect row indices, then delete in one pass
- âš ï¸ **Per-row FK checks**: Checks referential integrity individually for each row
- âš ï¸ **Row collection overhead**: Stores `Vec<(usize, Row)>` for all deletions
- âš ï¸ **Cell-based indexing**: Uses `Cell<usize>` for index tracking during deletion

**Code snippet** (delete/executor.rs:115-160):
```rust
// Try primary key optimization
if let Some(pk_values) = Self::extract_primary_key_lookup(where_expr, &schema) {
    if let Some(pk_index) = table.primary_key_index() {
        if let Some(&row_index) = pk_index.get(&pk_values) {
            rows_and_indices_to_delete.push((row_index, table.scan()[row_index].clone()));
        }
    }
}

// Check referential integrity for each row
for (_, row) in &rows_and_indices_to_delete {
    check_no_child_references(database, &stmt.table_name, row)?;
}

// Delete rows using indices
let indices_to_delete: std::collections::HashSet<usize> =
    rows_and_indices_to_delete.iter().map(|(idx, _)| *idx).collect();

let current_index = Cell::new(0);
let deleted_count = table_mut.delete_where(|_row| {
    let index = current_index.get();
    let should_delete = indices_to_delete.contains(&index);
    current_index.set(index + 1);
    should_delete
});
```

### SQLite3 Implementation

**Files**:
- `src/delete.c` - Main DELETE logic
- `src/vdbe.c` - Bytecode execution
- `src/btree.c` - B-tree deletions

**Architecture**:
```
sqlite3DeleteFrom()
â”œâ”€â”€ Parse and validate DELETE statement
â”œâ”€â”€ Check for optimization opportunities
â”‚   â”œâ”€â”€ Truncate optimization (DELETE without WHERE)
â”‚   â””â”€â”€ Index-based deletion
â”œâ”€â”€ Generate VDBE bytecode program
â””â”€â”€ Execute bytecode:
    â”œâ”€â”€ OpenWrite - Open table B-tree cursor
    â”œâ”€â”€ For each row matching WHERE:
    â”‚   â”œâ”€â”€ Seek/ScanNext - Find row via index or scan
    â”‚   â”œâ”€â”€ Check FK constraints (if needed)
    â”‚   â””â”€â”€ Delete - Remove from B-tree
    â”‚       â”œâ”€â”€ Mark cell as deleted (tombstone)
    â”‚       â””â”€â”€ Potentially rebalance B-tree
    â””â”€â”€ Close
```

**Key characteristics**:
- âœ… **Index-optimized access**: Uses covering indexes when available
- âœ… **Truncate optimization**: Fast path for unqualified DELETE
- âœ… **Lazy tombstones**: Marks cells as deleted, defers B-tree rebalancing
- âœ… **Batch FK validation**: Can batch FK checks in some cases
- âœ… **Zero-row optimization**: Early exit if WHERE predicate provably empty

**Pseudocode** (simplified from delete.c):
```c
// Generate bytecode for DELETE
void sqlite3DeleteFrom(...) {
    // ... parse and setup ...

    // Optimize for WHERE clause
    if (canUseCoveringIndex) {
        OpenRead(indexCursor, indexRootPage);
        while (NextRow(indexCursor)) {
            rowid = Column(indexCursor, rowidColumn);
            Delete(tableCursor, rowid);  // Tombstone + deferred rebalance
        }
    } else {
        OpenRead(tableCursor, tableRootPage);
        while (NextRow(tableCursor)) {
            if (evaluateWhere()) {
                Delete(tableCursor, CurrentRowid());
            }
        }
    }
}
```

### Performance Comparison

| Aspect | vibesql | SQLite3 | Winner | Notes |
|--------|---------|---------|--------|-------|
| **Row lookup** | PK index or full scan | Query planner + covering index | SQLite | SQLite can use non-PK indexes efficiently |
| **Deletion strategy** | Two-phase (collect â†’ delete) | Streaming tombstones | SQLite | vibesql allocates Vec for indices; SQLite marks in-place |
| **FK constraint checks** | Per-row upfront | Batched where possible | SQLite | vibesql checks each row individually |
| **Memory overhead** | O(deleted_rows) for collection | O(1) streaming | SQLite | vibesql stores row copies + indices |
| **Truncate optimization** | âœ… Implemented (100-1000x) | âœ… Implemented | Tie | Both have fast path for DELETE without WHERE |
| **B-tree rebalancing** | Immediate on each delete | Deferred/batched | SQLite | SQLite amortizes rebalancing cost |

**vibesql overhead sources**:
1. **Row collection**: Stores `Vec<(usize, Row)>` instead of streaming deletions
2. **Per-row FK checks**: No batching or optimization
3. **Index â†’ HashSet conversion**: Extra allocation for `indices_to_delete`
4. **Cell-based iteration**: `Cell<usize>` for tracking current index adds indirection

**Optimization opportunities**:
- ğŸ”§ **Streaming deletions**: Delete rows as they're found instead of collecting first
- ğŸ”§ **Batch FK checks**: Group FK validations by referenced table
- ğŸ”§ **Skip row cloning**: Only need indices, not full row copies for FK checks
- ğŸ”§ **Direct index iteration**: Avoid `Cell<usize>` by using storage API that tracks indices
- ğŸ”§ **Covering index support**: Use non-PK indexes for efficient lookups

---

## 4. SELECT with WHERE Clause

### Benchmark Profile
- **Test**: `SELECT * FROM table WHERE id < threshold` (filters 10% of rows)
- **Scales**: 1K, 10K, 100K rows total
- **Expected bottlenecks**: Row scanning, predicate evaluation, result materialization

### vibesql Implementation

**Files**:
- `crates/executor/src/select/executor.rs` - Main SELECT logic
- `crates/executor/src/select/scan.rs` - Table scanning
- `crates/executor/src/select/filter.rs` - WHERE clause evaluation

**Architecture**:
```
SelectExecutor::execute()
â”œâ”€â”€ Privilege check (SELECT privilege)
â”œâ”€â”€ Process FROM clause â†’ get table(s)
â”œâ”€â”€ Table scan â†’ rows iterator
â”œâ”€â”€ Apply WHERE clause filter
â”‚   â””â”€â”€ ExpressionEvaluator::evaluate() per row
â”œâ”€â”€ Apply projection (SELECT list)
â”œâ”€â”€ Apply DISTINCT (if present)
â”œâ”€â”€ Apply GROUP BY + aggregates
â”œâ”€â”€ Apply HAVING clause
â”œâ”€â”€ Apply ORDER BY
â””â”€â”€ Apply LIMIT/OFFSET
```

**Key characteristics**:
- âœ… **Pipelined execution**: Filter â†’ project â†’ group â†’ order in stages
- âš ï¸ **No index optimization**: WHERE clause always uses full table scan
- âš ï¸ **Per-row evaluation**: `ExpressionEvaluator::evaluate()` called for each row
- âš ï¸ **Result materialization**: All filtered rows collected before LIMIT

**Code snippet** (conceptual from select/filter.rs):
```rust
pub fn filter_rows(
    rows: Vec<storage::Row>,
    where_clause: &Option<ast::WhereClause>,
    evaluator: &ExpressionEvaluator,
) -> Result<Vec<storage::Row>, ExecutorError> {
    match where_clause {
        None => Ok(rows), // No filter
        Some(ast::WhereClause::Condition(expr)) => {
            // Filter rows based on WHERE expression
            rows.into_iter()
                .filter(|row| {
                    match evaluator.evaluate_bool(expr, row) {
                        Ok(true) => true,
                        _ => false,
                    }
                })
                .collect()
        }
    }
}
```

### SQLite3 Implementation

**Files**:
- `src/select.c` - SELECT statement processing
- `src/where.c` - WHERE clause optimization (query planner)
- `src/vdbe.c` - Bytecode execution

**Architecture**:
```
sqlite3Select()
â”œâ”€â”€ Parse SELECT statement
â”œâ”€â”€ Query planner:
â”‚   â”œâ”€â”€ Analyze WHERE clause
â”‚   â”œâ”€â”€ Choose optimal index(es)
â”‚   â”œâ”€â”€ Estimate costs
â”‚   â””â”€â”€ Generate access plan
â”œâ”€â”€ Generate VDBE bytecode
â””â”€â”€ Execute bytecode:
    â”œâ”€â”€ Use index scan (if chosen by planner)
    â”‚   â”œâ”€â”€ SeekGE/SeekLE - Position cursor
    â”‚   â””â”€â”€ Iterate only matching rows
    â”œâ”€â”€ Or use table scan (if no index)
    â”œâ”€â”€ Early exit for LIMIT
    â””â”€â”€ Materialize results
```

**Key characteristics**:
- âœ… **Query planner**: Cost-based optimization of access paths
- âœ… **Index range scans**: For `WHERE id < threshold`, uses index if available
- âœ… **Bytecode compilation**: WHERE predicate compiled to bytecode
- âœ… **Early exit**: LIMIT stops execution immediately
- âœ… **Covering indexes**: Can satisfy query entirely from index

**Pseudocode** (simplified from where.c + select.c):
```c
// Query planner chooses index for WHERE
WhereLoop* planWhere(...) {
    // Analyze WHERE clause
    if (hasIndexOnColumn(idColumn)) {
        // Use index range scan: O(log N + K) where K = matching rows
        return createIndexScanPlan(idIndex, LT, threshold);
    } else {
        // Fall back to table scan: O(N)
        return createTableScanPlan();
    }
}

// Execute using chosen plan
if (useIndexScan) {
    SeekLE(indexCursor, threshold);
    while (NotAtEnd(indexCursor)) {
        rowid = ReadIndexValue(indexCursor);
        Seek(tableCursor, rowid);
        OutputRow(tableCursor);
        Next(indexCursor);
        if (reachedLimit) break;  // Early exit
    }
}
```

### Performance Comparison

| Aspect | vibesql | SQLite3 | Winner | Notes |
|--------|---------|---------|--------|-------|
| **Query planning** | None (always table scan) | Cost-based planner | SQLite | Massive difference: O(N) vs O(log N + K) |
| **Index usage** | Not implemented | Range scans, covering indexes | SQLite | vibesql always scans all rows |
| **WHERE evaluation** | Interpreted per row | Compiled bytecode | SQLite | Bytecode has lower overhead |
| **LIMIT optimization** | Filters all rows first | Early exit | SQLite | vibesql materializes all before LIMIT |
| **Result building** | Materializes all rows | Streaming/early exit | SQLite | vibesql collects into Vec |

**vibesql overhead sources**:
1. **No index support**: O(N) table scan for every query, even with PK lookups
2. **No query planner**: Can't optimize access paths
3. **No early exit**: LIMIT applied after filtering all rows
4. **Full materialization**: Collects all filtered rows before projection/limit

**Optimization opportunities (Major Impact)**:
- ğŸ”§ğŸ”§ğŸ”§ **Index support for WHERE**: Implement B-tree index range scans (biggest opportunity)
- ğŸ”§ğŸ”§ **Query planner**: Cost-based selection of access paths
- ğŸ”§ **Early LIMIT exit**: Stop scanning once LIMIT rows found
- ğŸ”§ **Bytecode compilation**: Compile WHERE predicates to reduce evaluation overhead
- ğŸ”§ **Streaming execution**: Avoid materializing all rows before LIMIT

**Note**: This is likely the **biggest performance gap** between vibesql and SQLite for this benchmark. SQLite's `WHERE id < threshold` uses index range scan (O(log N + K)), while vibesql scans all N rows.

---

## 5. Aggregate Operations (COUNT, SUM, AVG)

### Benchmark Profile
- **Test**: `SELECT COUNT(*) FROM table`, `SELECT SUM(value)`, `SELECT AVG(value)`
- **Scales**: 1K, 10K, 100K rows
- **Expected bottlenecks**: Full table scan, accumulator overhead, DISTINCT tracking

### vibesql Implementation

**File**: `crates/executor/src/select/grouping.rs`

**Architecture**:
```
Process aggregates:
â”œâ”€â”€ Full table scan (no optimization for COUNT(*))
â”œâ”€â”€ For each row:
â”‚   â”œâ”€â”€ Evaluate aggregate expression
â”‚   â””â”€â”€ AggregateAccumulator::accumulate(value)
â”‚       â”œâ”€â”€ COUNT: increment counter, track DISTINCT set
â”‚       â”œâ”€â”€ SUM: add_sql_values(), track DISTINCT set
â”‚       â”œâ”€â”€ AVG: add to sum, increment count, track DISTINCT
â”‚       â”œâ”€â”€ MIN: compare_sql_values(), track DISTINCT
â”‚       â””â”€â”€ MAX: compare_sql_values(), track DISTINCT
â””â”€â”€ Finalize: AggregateAccumulator::finalize()
    â””â”€â”€ AVG: divide_sql_value(sum, count)
```

**Key characteristics**:
- âœ… **Comprehensive type support**: Handles all SQL numeric types correctly
- âœ… **DISTINCT support**: HashSet tracking for DISTINCT aggregates
- âš ï¸ **No COUNT(*) optimization**: Scans all rows even for COUNT(*)
- âš ï¸ **No index COUNT optimization**: Can't use index node counts
- âš ï¸ **Per-row type checking**: Runtime type checks in accumulate()

**Code snippet** (grouping.rs:40-184):
```rust
pub(super) enum AggregateAccumulator {
    Count { count: i64, distinct: bool, seen: Option<HashSet<types::SqlValue>> },
    Sum { sum: types::SqlValue, distinct: bool, seen: Option<HashSet<types::SqlValue>> },
    Avg { sum: types::SqlValue, count: i64, distinct: bool, seen: Option<HashSet<types::SqlValue>> },
    Min { value: Option<types::SqlValue>, distinct: bool, seen: Option<HashSet<types::SqlValue>> },
    Max { value: Option<types::SqlValue>, distinct: bool, seen: Option<HashSet<types::SqlValue>> },
}

pub(super) fn accumulate(&mut self, value: &types::SqlValue) {
    match self {
        AggregateAccumulator::Count { ref mut count, distinct, seen } => {
            if value.is_null() {
                return;
            }
            if *distinct {
                if seen.as_mut().unwrap().insert(value.clone()) {
                    *count += 1;
                }
            } else {
                *count += 1;  // Simple increment
            }
        }
        AggregateAccumulator::Sum { ref mut sum, distinct, seen } => {
            match value {
                types::SqlValue::Null => {}
                types::SqlValue::Integer(_) | /* ... all numeric types */ => {
                    if *distinct {
                        if seen.as_mut().unwrap().insert(value.clone()) {
                            *sum = add_sql_values(sum, value);  // Runtime dispatch
                        }
                    } else {
                        *sum = add_sql_values(sum, value);
                    }
                }
                _ => {}
            }
        }
        // ... AVG, MIN, MAX similar patterns
    }
}
```

### SQLite3 Implementation

**Files**:
- `src/select.c` - Aggregate query processing
- `src/vdbemem.c` - Value accumulation
- `src/func.c` - Built-in aggregate functions

**Architecture**:
```
Process aggregates:
â”œâ”€â”€ Optimization: COUNT(*) with no WHERE
â”‚   â””â”€â”€ Return nEntry from B-tree metadata (O(1))
â”œâ”€â”€ Optimization: COUNT(*) with WHERE + index
â”‚   â””â”€â”€ Count index entries (no row fetches)
â”œâ”€â”€ Normal path:
â”‚   â”œâ”€â”€ Table/index scan
â”‚   â”œâ”€â”€ For each row:
â”‚   â”‚   â””â”€â”€ Aggregate function bytecode
â”‚   â”‚       â”œâ”€â”€ COUNT: increment register
â”‚   â”‚       â”œâ”€â”€ SUM: add to register (typed arithmetic)
â”‚   â”‚       â”œâ”€â”€ AVG: sum + count registers
â”‚   â”‚       â”œâ”€â”€ MIN/MAX: compare + update register
â”‚   â””â”€â”€ Finalize aggregate value
```

**Key characteristics**:
- âœ… **COUNT(*) optimization**: O(1) for unqualified COUNT(*) using B-tree metadata
- âœ… **Index COUNT**: Uses index statistics when possible
- âœ… **Compiled aggregation**: Aggregate logic compiled to bytecode
- âœ… **Covering index aggregates**: Can compute MIN/MAX from index alone
- âœ… **Register-based accumulation**: Direct register operations, no allocations

**Pseudocode** (simplified from select.c + func.c):
```c
// Special case: COUNT(*) with no WHERE
if (isCountStar && noWhereClause) {
    return btree->nEntry;  // O(1) - just return row count
}

// Special case: COUNT(*) with index
if (isCountStar && canUseCoveringIndex) {
    count = 0;
    while (NextIndexEntry()) {
        count++;  // No row fetch needed
    }
    return count;
}

// Normal aggregate path
AggContext aggCtx;
aggCtx.count = 0;
aggCtx.sum = 0;

while (NextRow()) {
    value = EvaluateExpression();

    // Inlined aggregate logic in bytecode
    switch (aggFunc) {
        case COUNT:
            if (!IsNull(value)) aggCtx.count++;
            break;
        case SUM:
            aggCtx.sum += GetNumeric(value);  // Type-specific add
            break;
        case AVG:
            aggCtx.sum += GetNumeric(value);
            aggCtx.count++;
            break;
    }
}

// Finalize
switch (aggFunc) {
    case AVG:
        return aggCtx.sum / aggCtx.count;
    // ...
}
```

### Performance Comparison

| Aspect | vibesql | SQLite3 | Winner | Notes |
|--------|---------|---------|--------|-------|
| **COUNT(*) optimization** | None (scans all rows) | O(1) using B-tree metadata | SQLite | Huge difference: O(N) vs O(1) |
| **Index-based aggregates** | Not implemented | MIN/MAX from index, COUNT from index | SQLite | SQLite can avoid table access entirely |
| **Accumulation overhead** | Enum match + function calls | Direct register operations | SQLite | vibesql has pattern matching + dispatch overhead |
| **Type handling** | Runtime type checking per row | Compiled type-specific code | SQLite | Bytecode has type info compiled in |
| **DISTINCT tracking** | HashSet allocations | Similar approach | Tie | Both use hash-based deduplication |
| **Memory allocations** | Per-aggregate HashSet (if DISTINCT) | Minimal (registers) | SQLite | vibesql may allocate for DISTINCT |

**vibesql overhead sources**:
1. **No COUNT(*) optimization**: Scans all N rows instead of O(1) metadata lookup
2. **No index usage**: Can't extract MIN/MAX from index
3. **Enum dispatch**: `match self` on AggregateAccumulator adds overhead
4. **Function call overhead**: `add_sql_values()`, `compare_sql_values()` are function calls
5. **Type checking**: Runtime checks in `accumulate()` per row

**Optimization opportunities (Major Impact for COUNT)**:
- ğŸ”§ğŸ”§ğŸ”§ **COUNT(*) optimization**: Return table row count from metadata (O(1) vs O(N))
- ğŸ”§ğŸ”§ **Index aggregates**: Implement MIN/MAX extraction from indexes
- ğŸ”§ğŸ”§ **Index COUNT**: Count index entries instead of fetching rows
- ğŸ”§ **Inline accumulation**: Reduce function call overhead
- ğŸ”§ **Type-specialized accumulators**: Compile different code paths for Integer vs Float vs Numeric

**Note**: COUNT(*) optimization is especially important - it's a common query that could be O(1) but is currently O(N).

---

## Summary & Optimization Opportunities

### Overall Architecture Comparison

| Dimension | vibesql | SQLite3 |
|-----------|---------|---------|
| **Execution model** | Direct Rust execution | VDBE bytecode interpreter |
| **Query optimization** | Minimal (PK lookups only) | Cost-based query planner |
| **Storage layer** | HashMap-based in-memory | B-tree pages (cache-optimized) |
| **Index support** | Primary key only (HashMap) | B-tree indexes (multiple per table) |
| **Constraint checking** | Upfront validation, separate validators | Integrated into storage operations |
| **Memory strategy** | Two-phase (collect then execute) | Streaming execution |

### Performance Gap Analysis

Based on code analysis, here are the expected performance ratios (vibesql vs SQLite3):

| Operation | Expected Ratio | Primary Bottleneck |
|-----------|----------------|-------------------|
| **INSERT (1K-100K)** | 2-4x slower | Two-phase validation, HashMap vs B-tree, allocations |
| **UPDATE (1K-100K)** | 3-6x slower | Per-row validators, no in-place updates, FK over-checking |
| **DELETE (1K-100K)** | 2-5x slower | Row collection overhead, per-row FK checks |
| **SELECT WHERE (10%)** | **10-100x slower** | **No index support** (O(N) vs O(log N + K)) |
| **COUNT(*)** | **10-100x slower** | **No COUNT(*) optimization** (O(N) vs O(1)) |
| **SUM/AVG** | 2-4x slower | Enum dispatch, no index usage, function call overhead |

### Top Optimization Priorities

#### Tier 1: Highest Impact (10-100x potential improvement)

1. **Implement index-based WHERE clause evaluation**
   - **Impact**: SELECT WHERE could improve from O(N) to O(log N + K)
   - **Benchmark**: SELECT WHERE 10% of 100K rows: 100x speedup potential
   - **Effort**: High (requires B-tree index implementation)
   - **Files to modify**: `select/scan.rs`, `select/filter.rs`, new `index/` module

2. **Add COUNT(*) optimization**
   - **Impact**: COUNT(*) with no WHERE becomes O(1) from O(N)
   - **Benchmark**: COUNT on 100K rows: 100,000x speedup potential
   - **Effort**: Low (just return table row count metadata)
   - **Files to modify**: `select/grouping.rs`

3. **Implement query planner**
   - **Impact**: Enables index selection, join ordering, early exits
   - **Benchmark**: All SELECT queries benefit
   - **Effort**: Very High (foundational change)
   - **Files to create**: `planner/` module

#### Tier 2: Moderate Impact (2-5x potential improvement)

4. **Reduce constraint validation overhead**
   - **Impact**: INSERT, UPDATE faster by reusing validators
   - **Benchmark**: INSERT 100K: 2x speedup potential
   - **Effort**: Low (reuse validators outside loops)
   - **Files to modify**: `insert/execution.rs`, `update/mod.rs`

5. **Implement streaming execution**
   - **Impact**: Reduce memory allocations in INSERT/UPDATE/DELETE
   - **Benchmark**: All write operations: 1.5-2x speedup
   - **Effort**: Medium (refactor two-phase to pipeline)
   - **Files to modify**: All executor modules

6. **Add in-place UPDATE support**
   - **Impact**: UPDATEs that don't change row size avoid copy
   - **Benchmark**: UPDATE 100K: 2x speedup potential
   - **Effort**: Medium (storage layer changes)
   - **Files to modify**: `update/mod.rs`, `storage/` crate

7. **Optimize FK validation**
   - **Impact**: Skip FK checks when FK columns unchanged
   - **Benchmark**: UPDATE 100K: 1.5-2x speedup
   - **Effort**: Low (add column change tracking)
   - **Files to modify**: `update/foreign_keys.rs`

#### Tier 3: Incremental Improvements (1.2-2x potential improvement)

8. **Reduce allocations in INSERT**
   - **Impact**: Use arena allocator or pre-sized buffers
   - **Benchmark**: INSERT 100K: 1.5x speedup
   - **Effort**: Medium (allocator refactoring)
   - **Files to modify**: `insert/execution.rs`

9. **Batch INSERT API**
   - **Impact**: Amortize HashMap operations across batch
   - **Benchmark**: INSERT 100K: 1.5-2x speedup
   - **Effort**: Medium (storage API changes)
   - **Files to modify**: `insert/execution.rs`, `storage/` crate

10. **Inline aggregate accumulation**
    - **Impact**: Reduce function call overhead
    - **Benchmark**: SUM/AVG on 100K: 1.3x speedup
    - **Effort**: Low (inline functions)
    - **Files to modify**: `select/grouping.rs`

### Measurement Validation

To validate these hypotheses, run the actual benchmarks:

```bash
# Build vibesql Python bindings
cd crates/python-bindings && maturin build --release
pip install target/wheels/vibesql-*.whl

# Run microbenchmarks
pytest benchmarks/test_micro_benchmarks.py --benchmark-only

# Compare results
python scripts/compare_performance.py
```

Expected output will show actual performance ratios that can be compared to these predictions.

---

## Appendix: Code Location Reference

### vibesql Key Files

```
crates/executor/src/
â”œâ”€â”€ insert/
â”‚   â”œâ”€â”€ execution.rs          # Main INSERT logic, two-phase validation
â”‚   â”œâ”€â”€ row_validator.rs      # Constraint validation
â”‚   â”œâ”€â”€ bulk_transfer.rs      # INSERT...SELECT optimization
â”‚   â””â”€â”€ constraints.rs        # NOT NULL, CHECK, etc.
â”œâ”€â”€ update/
â”‚   â”œâ”€â”€ mod.rs                # Main UPDATE logic, two-phase execution
â”‚   â”œâ”€â”€ row_selector.rs       # WHERE clause + PK optimization
â”‚   â”œâ”€â”€ value_updater.rs      # Assignment application
â”‚   â”œâ”€â”€ constraints.rs        # Constraint validation
â”‚   â””â”€â”€ foreign_keys.rs       # FK validation
â”œâ”€â”€ delete/
â”‚   â”œâ”€â”€ executor.rs           # Main DELETE logic, PK optimization
â”‚   â””â”€â”€ integrity.rs          # FK referential integrity
â”œâ”€â”€ select/
â”‚   â”œâ”€â”€ executor.rs           # Main SELECT orchestration
â”‚   â”œâ”€â”€ scan.rs               # Table scanning
â”‚   â”œâ”€â”€ filter.rs             # WHERE clause filtering
â”‚   â”œâ”€â”€ grouping.rs           # GROUP BY + aggregates
â”‚   â”œâ”€â”€ order.rs              # ORDER BY
â”‚   â””â”€â”€ projection.rs         # SELECT list evaluation
â””â”€â”€ evaluator/
    â””â”€â”€ mod.rs                # Expression evaluation (WHERE, assignments, etc.)
```

### SQLite3 Key Files (for reference)

```
src/
â”œâ”€â”€ insert.c                  # INSERT statement execution
â”œâ”€â”€ update.c                  # UPDATE statement execution
â”œâ”€â”€ delete.c                  # DELETE statement execution
â”œâ”€â”€ select.c                  # SELECT statement processing
â”œâ”€â”€ where.c                   # Query planner and WHERE optimization
â”œâ”€â”€ vdbe.c                    # Virtual Database Engine (bytecode execution)
â”œâ”€â”€ vdbeaux.c                 # VDBE helper functions
â”œâ”€â”€ vdbemem.c                 # Memory management for VDBE
â”œâ”€â”€ btree.c                   # B-tree storage implementation
â”œâ”€â”€ func.c                    # Built-in SQL functions (including aggregates)
â””â”€â”€ expr.c                    # Expression evaluation
```

---

**Document Version**: 1.0
**Last Updated**: 2025-11-06
**Next Steps**: Run actual benchmarks to validate performance hypotheses and prioritize optimizations based on measured impact.
