name: Performance Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: read

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache Rust dependencies and build artifacts
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "rust-ci-cache"  # Shared with test.yml and deploy-demo.yml

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install uv
        uses: astral-sh/setup-uv@v3

      - name: Build and install Python bindings
        run: |
          uv pip install --system maturin
          cd crates/python-bindings
          maturin build --release
          uv pip install --system ../../target/wheels/nistmemsql_py-*.whl

      - name: Install benchmark dependencies
        run: |
          # Install requirements except nistmemsql (already installed from wheel)
          grep -v "^nistmemsql" benchmarks/requirements.txt > /tmp/requirements-ci.txt
          uv pip install --system -r /tmp/requirements-ci.txt

      - name: Run benchmarks (fast for PRs, full for main)
        working-directory: benchmarks
        env:
          # Skip 20k tests in PRs, run full suite on main
          BENCHMARK_SKIP_LARGE: ${{ github.event_name == 'pull_request' && 'true' || 'false' }}
        run: |
          if [ "$BENCHMARK_SKIP_LARGE" = "true" ]; then
            echo "Running fast benchmark suite (skipping 20k tests for PRs)"
            pytest test_insert.py test_update.py test_delete.py test_select.py test_aggregates.py \
              --benchmark-only \
              --benchmark-json=../benchmark_results.json \
              --benchmark-min-rounds=3 \
              --benchmark-warmup=off \
              -k "not 20k"
          else
            echo "Running full benchmark suite"
            pytest test_insert.py test_update.py test_delete.py test_select.py test_aggregates.py \
              --benchmark-only \
              --benchmark-json=../benchmark_results.json \
              --benchmark-min-rounds=5 \
              --benchmark-warmup=on
          fi

      - name: Download baseline benchmarks (for PRs)
        if: github.event_name == 'pull_request'
        continue-on-error: true
        run: |
          # Try to download baseline from main branch artifacts
          gh run download \
            --repo ${{ github.repository }} \
            --branch main \
            --name benchmark-results \
            --dir baseline \
            || echo "No baseline found, skipping regression check"
        env:
          GH_TOKEN: ${{ github.token }}

      - name: Check for performance regressions
        if: github.event_name == 'pull_request' && hashFiles('baseline/benchmark_results.json') != ''
        run: python benchmarks/check_regression.py benchmark_results.json baseline/benchmark_results.json
        continue-on-error: true

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark_results.json
          retention-days: 90

      - name: Display results summary
        run: |
          echo "## Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          python benchmarks/format_results.py benchmark_results.json >> $GITHUB_STEP_SUMMARY
