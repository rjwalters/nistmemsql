name: Deploy Web Demo and Badges

on:
  # Run after test and benchmark workflows complete
  workflow_run:
    workflows: ["Tests and Coverage", "Performance Benchmarks"]
    types: [completed]
    branches: [main]
  # Also allow manual triggering
  workflow_dispatch:

# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      # Setup Rust for WASM build
      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          targets: wasm32-unknown-unknown

      - name: Cache Rust dependencies and build artifacts
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "rust-ci-cache"  # Shared with test.yml and benchmarks.yml

      # Install wasm-pack for building WASM bindings
      - name: Install wasm-pack
        run: curl https://rustwasm.github.io/wasm-pack/installer/init.sh -sSf | sh

      # Build WASM bindings
      - name: Build WASM bindings
        working-directory: crates/wasm-bindings
        run: wasm-pack build --target web --release

      # Setup Node.js for web demo
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: web-demo/package-lock.json

      # Install web demo dependencies
      - name: Install dependencies
        working-directory: web-demo
        run: npm ci

      # Build web demo
      - name: Build web demo
        working-directory: web-demo
        run: npm run build

      # Try to download conformance test results from test workflow
      # This avoids re-running expensive conformance tests on main branch deploys
      # Falls back to running tests if artifacts unavailable (e.g., first run)
      - name: Download conformance test results
        continue-on-error: true
        run: |
          # Try to download from the latest successful test workflow run on main
          gh run download \
            --repo ${{ github.repository }} \
            --branch main \
            --name test-results \
            --dir target \
            || echo "No test results found, will run tests"
        env:
          GH_TOKEN: ${{ github.token }}

      # Run conformance tests only if download failed
      - name: Run sqltest conformance suite
        if: hashFiles('target/sqltest_results.json') == ''
        run: cargo test --test sqltest_conformance --release -- --nocapture
        continue-on-error: true

      # Try to download benchmark results from benchmark workflow
      # This avoids re-running expensive benchmarks on main branch deploys
      # Falls back to running benchmarks if artifacts unavailable
      - name: Download benchmark results
        continue-on-error: true
        run: |
          echo "Attempting to download benchmark results artifact..."
          # Try to download from the latest successful benchmark workflow run on main
          gh run download \
            --repo ${{ github.repository }} \
            --branch main \
            --name benchmark-results \
            && echo "✓ Successfully downloaded benchmark results" \
            || echo "✗ No benchmark results found, will run benchmarks"

          # Check if file exists after download
          if [ -f "benchmark_results.json" ]; then
            echo "✓ benchmark_results.json found ($(wc -c < benchmark_results.json) bytes)"
            ls -lh benchmark_results.json
          else
            echo "✗ benchmark_results.json not found, benchmarks will run"
          fi
        env:
          GH_TOKEN: ${{ github.token }}

      # Only run benchmarks if download failed
      - name: Setup Python for benchmarks
        if: hashFiles('benchmark_results.json') == ''
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install uv
        if: hashFiles('benchmark_results.json') == ''
        uses: astral-sh/setup-uv@v3

      # Build and install Python bindings
      - name: Build and install Python bindings
        if: hashFiles('benchmark_results.json') == ''
        run: |
          uv pip install --system maturin
          cd crates/python-bindings
          maturin build --release
          uv pip install --system ../../target/wheels/nistmemsql_py-*.whl

      # Run benchmarks
      - name: Install benchmark dependencies
        if: hashFiles('benchmark_results.json') == ''
        run: |
          # Install requirements except nistmemsql (already installed from wheel)
          grep -v "^nistmemsql" benchmarks/requirements.txt > /tmp/requirements-ci.txt
          uv pip install --system -r /tmp/requirements-ci.txt

      - name: Run benchmarks
        if: hashFiles('benchmark_results.json') == ''
        working-directory: benchmarks
        run: |
          pytest test_insert.py test_update.py test_delete.py test_select.py test_aggregates.py \
            --benchmark-only \
            --benchmark-json=../benchmark_results.json \
            --benchmark-min-rounds=3 \
            --benchmark-warmup=off
        continue-on-error: true

      # Download existing benchmark history from gh-pages
      - name: Download benchmark history
        continue-on-error: true
        run: |
          curl -sL https://rjwalters.github.io/nistmemsql/benchmarks/benchmark_history.json \
            -o benchmark_history.json || echo "No existing history found"

      # Update benchmark history
      - name: Update benchmark history
        if: hashFiles('benchmark_results.json') != ''
        run: |
          python benchmarks/update_history.py \
            benchmark_results.json \
            benchmark_history.json \
            ${{ github.sha }}
        continue-on-error: true

      # Generate dynamic badge data
      - name: Generate badge JSON
        run: |
          # Create badges directory
          mkdir -p badges

          # Extract pass rate from sqltest results (default to 0.0 if not found)
          # Round to 1 decimal place for badge display
          if [ -f target/sqltest_results.json ]; then
            PASS_RATE=$(jq -r '.pass_rate // "0"' target/sqltest_results.json | xargs printf "%.1f")
          else
            PASS_RATE="0.0"
          fi

          # Determine badge color based on pass rate
          if (( $(echo "$PASS_RATE >= 80" | bc -l) )); then
            COLOR="brightgreen"
          elif (( $(echo "$PASS_RATE >= 60" | bc -l) )); then
            COLOR="green"
          elif (( $(echo "$PASS_RATE >= 40" | bc -l) )); then
            COLOR="yellow"
          elif (( $(echo "$PASS_RATE >= 20" | bc -l) )); then
            COLOR="orange"
          else
            COLOR="red"
          fi

          # Create badge JSON for shields.io endpoint
          cat > badges/sql1999-conformance.json <<JSON
          {
            "schemaVersion": 1,
            "label": "SQL:1999",
            "message": "${PASS_RATE}%",
            "color": "$COLOR"
          }
          JSON

          echo "Generated badge with pass rate: ${PASS_RATE}% (color: $COLOR)"

          # Copy full test results JSON for conformance page
          if [ -f target/sqltest_results.json ]; then
            cp target/sqltest_results.json badges/sqltest_results.json
            echo "Copied conformance test results to badges/"
          fi

          # Copy benchmark results
          if [ -f benchmark_results.json ]; then
            cp benchmark_results.json badges/benchmark_results.json
            echo "Copied benchmark results to badges/"
          fi

          # Copy benchmark history
          if [ -f benchmark_history.json ]; then
            cp benchmark_history.json badges/benchmark_history.json
            echo "Copied benchmark history to badges/"
          fi

      # Prepare deployment directory
      - name: Prepare deployment
        run: |
          # Create deployment directory
          mkdir -p deploy

          # Copy web demo build output
          cp -r web-demo/dist/* deploy/

          # Create benchmarks directory in deployment
          mkdir -p deploy/benchmarks

          # Copy badges to subdirectory (includes sqltest_results.json and benchmark_results.json)
          cp -r badges deploy/badges

          # Also copy benchmark results and history to /benchmarks for easier access
          if [ -f badges/benchmark_results.json ]; then
            cp badges/benchmark_results.json deploy/benchmarks/benchmark_results.json
          fi
          if [ -f badges/benchmark_history.json ]; then
            cp badges/benchmark_history.json deploy/benchmarks/benchmark_history.json
          fi

          echo "Deployment directory contents:"
          ls -la deploy/
          ls -la deploy/badges/

      # Upload to GitHub Pages
      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: './deploy'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
